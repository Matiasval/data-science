pacman::p_load()

data <- read_rds()
head(data)

str(data)


summary(data)

dim(data)

# en R base definimos el nuevo objeto data1 al especificar filas y columnas de la data original
# los corchetes cuadrados denotan [fila, columna]
data1 <- data[data$week_year >= 2012, c("week_year", "rank_number", "player_slug")]

dim(data1)

library(tidyverse)

data2 <- data %>% 
  filter(week_year >= 2012) %>% 
  select(week_year, rank_number, player_slug)

dim(data2)

unicos <- unique(data2)

dim(unicos)

# genero un listado de 1000 numeros aleatorios a partir de los valores entre 1 y el numero de registros unicos, sin reperirse
sampleIndex <- sample(1:nrow(unicos),1000, replace = F)

# extraigo las filas correspondientes al lista generado
sampleData <- unicos[sampleIndex,]

dim(sampleData)

# agrego la variable rank_number calculandole la mediana, para cada semana y jugador
agg_df <- aggregate(rank_number ~ week_year + player_slug, data1, mean)

dim(agg_df)

# calculamos el maximo valor anual con aggregate
max_rank <- aggregate(rank_number ~ week_year, agg_df, max)

# cambio el nombre de rank_number para que no se repita cuando fusione
colnames(max_rank)[2] <- "max"

# fusiono data agregada con los maximos, usando la semana como el id del cruce
agg_df <- merge(agg_df, max_rank, by="week_year")

# genero ranking relativo dividiendo ranking por el maximo de esa semana
agg_df$rank_relative <- agg_df$rank_number / agg_df$max

summary(agg_df)


# variable top10 es igual a una desigualdad, que retorna TRUE o FALSE. Al parsearlo como numeric, TRUE pasa a ser 1 y FALSE a 0
agg_df$top10 <- as.numeric(agg_df$rank_number <= 10)

table(agg_df$top10)


#filtro data para el grafico
data_plot <- agg_df %>% filter(top10 == 1) 

ggplot(data_plot, aes(week_year, rank_number, col = player_slug)) + 
  geom_line() + 
  geom_point() + 
  theme(legend.position = "bottom") +
  scale_y_reverse() + 
  ggtitle("Evolucion del top10 ATP entre 2012 y 2017") +
  scale_color_viridis_d()



library(tidyverse)
library(datasauRus)
library(proxy)

summary(datasaurus_dozen)

stats <- datasaurus_dozen %>% 
  group_by(dataset) %>% 
  summarize(
    mean_x    = mean(x),
    mean_y    = mean(y),
    std_dev_x = sd(x),
    std_dev_y = sd(y),
    corr_pears  = cor(x, y, method = "pearson"),
    corr_spear  = cor(x, y, method = "spearman"),
    corr_kendall  = cor(x, y, method = "kendall"),
    simil_cos = simil(list(x,y), method = "cosine") %>% as.numeric(),   # funcion simil en la libreria proxy
    simil_jac = simil(list(x,y), method = "Jaccard") %>% as.numeric(),   # funcion simil en la libreria proxy
    simil_sm = simil(list(x,y), method = "simple matching") %>% as.numeric(),   # funcion simil en la libreria proxy
    simil_kul = simil(list(x,y), method = "Kulczynski1") %>% as.numeric(),   # funcion simil en la libreria proxy
    dist_euc = dist(list(x,y), method = "Euclidean") %>% as.numeric(),   
    dist_manh = dist(list(x,y), method = "Manhattan") %>% as.numeric(),  
    dist_sup = dist(list(x,y), method = "supremum") %>% as.numeric(),   
    median_x    = median(x),
    median_y    = median(y),
    CV_x = sd(x) / mean(x),
    CV_y = sd(y) / mean(y),
    max_x = max(x),
    max_y = max(y)
  )

stats %>% glimpse()

## los metodos de similaridad y distancias disponibles en el paquete proxy
summary(pr_DB)

ggplot(datasaurus_dozen, aes(x=x, y=y, colour=dataset)) + # la primera linea define los parametros del grafico, la data, coordenadas y color
  geom_point() +                                        # en esta linea se define la geometria de la figura, en este caso un punto
  theme_void() +                                        # aca definimos el tema del grafico
  theme(legend.position = "none") +                     # quitamos la leyenda 
  facet_wrap(~dataset, ncol=3)                          # creamos un subgrafico por cada dataset  

ggplot(datasaurus_dozen, aes(x=x,colour=dataset))+
  geom_histogram(binwidth = 2)+                         # cambio la geometria
  theme_void()+
  theme(legend.position = "none")+
  facet_wrap(~dataset, ncol=3)

ggplot(datasaurus_dozen, aes(x=x,colour=dataset))+
  geom_density()+
  theme_void()+
  theme(legend.position = "none")+
  facet_wrap(~dataset, ncol=3)


ggplot(datasaurus_dozen, aes(x=x, colour=dataset))+
  geom_boxplot()+
  theme_void()+
  theme(legend.position = "none")+
  facet_wrap(~dataset, ncol=3)

ggplot(datasaurus_dozen, aes(x=x, y=y, colour=dataset))+
  geom_violin()+
  theme_void()+
  theme(legend.position = "none")+
  facet_wrap(~dataset, ncol=3)


# y podemos combinar geometrias entre si
ggplot(datasaurus_dozen, aes(x=x, y=dataset, colour=dataset)) +
  geom_point() +
  geom_boxplot() +
  theme_void() +
  theme(legend.position = "none")


iris0 <- iris %>% unique()

str(iris0)

# debemos transformar variable Species en numerica, lo hacemos creando variables dummy
iris0$setosa <- ifelse(iris0$Species == "setosa", 1, 0)
iris0$virginica <- ifelse(iris0$Species == "virginica", 1, 0)
iris0$versicolor <- ifelse(iris0$Species == "versicolor", 1, 0)

iris_num <- iris0 # creamos una copia de la data pero con varibles numericas solamente
iris_num$Species <- NULL




library(stuart) 

results <- bruteforce(iris_num, list(ra = names(iris_num)), 3,
                      cores = 1)  # numero de nucleos en la maquina

summary(results) 



library(FSinR)   # feature selection


searcher <- searchAlgorithm('geneticAlgorithm')
searcher <- searchAlgorithm('tabu', list(tamTabuList = 4, iter = 5, intensification=2, iterIntensification=5, diversification=1, iterDiversification=5, verbose=FALSE) )
searcher <- searchAlgorithm('antColony')
searcher <- searchAlgorithm('sequentialForwardSelection')
searcher <- searchAlgorithm('hillClimbing')




filtro <- filterEvaluator("IEConsistency")
filtro <- filterEvaluator('determinationCoefficient')
filtro <- filterEvaluator('chiSquared')
filtro <- filterEvaluator('MDLC') 



results <- featureSelection(iris0, 'Species', searcher, filtro)

results$bestFeatures


evaluator <- wrapperEvaluator("xgbLinear")
evaluator <- wrapperEvaluator("svmLinearWeights")
evaluator <- wrapperEvaluator("mlpWeightDecay")
evaluator <- wrapperEvaluator("lm")
evaluator <- wrapperEvaluator("knn")

results <- featureSelection(iris0, 'Species', searcher, evaluator)

results$bestFeatures


directSearcher <- directSearchAlgorithm('selectKBest', list(k=3))

results <- directFeatureSelection(iris0, 'Species', directSearcher, evaluator)

results$bestFeatures


library(GGally)

ggpairs(iris_num, aes(col=iris0$Species))



#PCA
PCA <- prcomp(iris_num)

barplot(PCA$sdev) ## graficamos el aporte de varianza de cada componente principal

predict(PCA) %>% as.data.frame() %>%  ggplot(aes(PC1,PC2, col=iris0$Species)) + geom_point()


#MDS
d <- dist(iris_num) # distancias euclidianas entre entidades
MDS <- cmdscale(d,eig=TRUE, k=2) # k es el numero de dimensiones de salida

MDS$points %>% as.data.frame() %>% ggplot(aes(V1,V2, col=iris0$Species)) + geom_point()


#nMDS
library(MASS)
nMDS <- isoMDS(d, k=2) 

nMDS$points %>% as.data.frame() %>% ggplot(aes(V1,V2, col=iris0$Species)) + geom_point()


#tSNE

library(Rtsne)
tsne <- Rtsne(iris_num, dims = 2, perplexity=30, max_iter = 500)

tsne$Y %>% as.data.frame() %>% ggplot(aes(V1,V2, col=iris0$Species)) + geom_point()


