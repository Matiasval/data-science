}
# probamos la funcion con diferentes parametros
gmm_func(data_tsne, 13, "VEE", 0.7)
# probamos la funcion con diferentes parametros
gmm_func(data_tsne, 13, "VEE", 0.2)
# probamos la funcion con diferentes parametros
gmm_func(data_tsne, 13, "VEE")
# probamos la funcion con diferentes parametros
gmm_func(data_tsne, 13, "VEE", 0.9)
# probamos la funcion con diferentes parametros
gmm_func(data_tsne, 13, "VII", 0.9)
# probamos la funcion con diferentes parametros
gmm_func(data_tsne, 4, "VII", 0.9)
# calculamos un modelo con TODAS las opciones
model_all <- Mclust(data_tsne)
# visualizamos el BIC de todas las opciones
plot(model_all, what = "BIC")
# vemos el resultado del modelo optimo
model_all
# visualizamos el modelo optimo
ggplot(data_tsne) +
aes(x=V1, y=V2, color=factor(model_all$classification)) +
geom_point(alpha=0.5)
# calculamos un modelo con TODAS las opciones
model_all <- Mclust(data_tsne, G=10:15)
# visualizamos el BIC de todas las opciones
plot(model_all, what = "BIC")
? cmeans
# la funcion cmeans recibe el parametro de fuzziness, admeas del numero de clusters
modelo_c_means <- cmeans(data_tsne, 13, m=3)
# exploramos las pertenencias de cada punto a cada cluster
modelo_c_means$membership %>% head()
# visualizamos los grupos obtenidos
ggplot(data_tsne, aes(V1, V2, color = factor(modelo_c_means$cluster))) +
geom_point() +
theme(legend.position = "none")
# creamos una funcion similar a la de GMM
cmeans_func <- function(data, k, fuzzifier, threshold=NULL){
model_gmm <-  cmeans(data, k, m=fuzzifier)
if(is.null(threshold)){
fuzz <- modelo_c_means$cluster
} else {
fuzz <- apply(modelo_c_means$membership, 1, function(x) ifelse(max(x) > threshold, which(x==max(x)),0))
}
ggplot(data[fuzz != 0,]) +
geom_point(aes(V1,V2, color=factor(fuzz[fuzz != 0]))) +
geom_point(data=data[fuzz == 0,], aes(V1,V2), alpha = 0.5, size=0.8) +
theme(legend.position = "none") +
ggtitle(paste0(round(nrow(data[fuzz == 0,])/nrow(data)*100,0),"% de los datos sin cluster"))
}
# probamos la funcion con diferentes parametros
cmeans_func(data_tsne, 13, 3)
# probamos la funcion con diferentes parametros
cmeans_func(data_tsne, 13, 3, 0.7)
# probamos la funcion con diferentes parametros
cmeans_func(data_tsne, 13, 3, 0.3)
# probamos la funcion con diferentes parametros
cmeans_func(data_tsne, 13, 4, 0.3)
# creamos una funcion para calcular el coeficiente de particion difusa a partir de las pertenencias
FPC <- function(membresias){
matriz <- membresias%*%t(membresias) # producto matricial
FPC <- sum(matriz*diag(nrow(matriz)))/nrow(matriz)
return(FPC)
}
# evaluamos el FPC del modelo cmeans
FPC(modelo_c_means$membership)
# evaluamos el FPC del modelo GMM
FPC(model_all$z)
# cargo la data y aplico los mismos tratamientos que en el caso de DBScan
data_tsne  <- read.csv("data/video_games_sales.csv") %>%
mutate(User_Score = as.numeric(User_Score)) %>%
filter(!(is.na(Critic_Score) | is.na(User_Score))) %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
unique()
# cargo librerias
pacman::p_load(tidyverse, Rtsne, mclust, e1071)
set.seed(42)
# cargo la data y aplico los mismos tratamientos que en el caso de DBScan
data_tsne  <- read.csv("data/video_games_sales.csv") %>%
mutate(User_Score = as.numeric(User_Score)) %>%
filter(!(is.na(Critic_Score) | is.na(User_Score))) %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
unique()
# cargo la data y aplico los mismos tratamientos que en el caso de DBScan
data_tsne  <- read.csv("data/video_games_sales.csv") %>%
mutate(User_Score = as.numeric(User_Score)) %>%
filter(!(is.na(Critic_Score) | is.na(User_Score))) %>%
#  select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
unique()
data_tsne  <- read.csv("data/video_games_sales.csv")
data_tsne  <- read.csv("data/video_games_sales.csv") %>%
mutate(User_Score = as.numeric(User_Score)) %>%
filter(!(is.na(Critic_Score) | is.na(User_Score)))
data_tsne <- data_cruda %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
#unique() %>%
Rtsne() %>%
.$Y %>%
as.data.frame()
# cargo la data y aplico los mismos tratamientos que en el caso de DBScan
data_cruda  <- read.csv("data/video_games_sales.csv") %>%
mutate(User_Score = as.numeric(User_Score)) %>%
filter(!(is.na(Critic_Score) | is.na(User_Score)))
data_tsne <- data_cruda %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
#unique() %>%
Rtsne() %>%
.$Y %>%
as.data.frame()
data_tsne <- data_cruda %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>% duplicated()
sum(data_tsne)
# cargo la data y aplico los mismos tratamientos que en el caso de DBScan
data_cruda  <- read.csv("data/video_games_sales.csv") %>%
mutate(User_Score = as.numeric(User_Score)) %>%
filter(!(is.na(Critic_Score) | is.na(User_Score)),
!duplicated())
which(data_tsne)
duplicados <- data_cruda %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
duplicated()
data_sin_dupes  <- data_cruda %>%
filter(duplicados)
data_sin_dupes  <- data_cruda %>%
filter(!duplicados)
data_tsne <- data_sin_dupes %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
Rtsne() %>%
.$Y %>%
as.data.frame()
# exploramos graficamente la data
ggplot(data_tsne) +
geom_point(aes(V1,V2))
# cargo librerias
pacman::p_load(tidyverse, Rtsne, mclust, e1071)
set.seed(42)
# cargo la data y aplico los mismos tratamientos que en el caso de DBScan
data_cruda  <- read.csv("data/video_games_sales.csv") %>%
mutate(User_Score = as.numeric(User_Score)) %>%
filter(!(is.na(Critic_Score) | is.na(User_Score)))
duplicados <- data_cruda %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
duplicated()
data_sin_dupes  <- data_cruda %>%
filter(!duplicados)
data_tsne <- data_sin_dupes %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
Rtsne() %>%
.$Y %>%
as.data.frame()
# exploramos graficamente la data
ggplot(data_tsne) +
geom_point(aes(V1,V2))
# uso distancia euclidiana
tempDist <- dist(data_tsne) %>% as.matrix()
modelo_kmeans <- kmeans(data_tsne, 13)
#reordeno filas y columnas en base al cluster obtenido
index <- sort(modelo_kmeans$cluster, index.return=TRUE)
tempDist <- tempDist[index$ix,index$ix]
rownames(tempDist) <- c(1:nrow(data))
rownames(tempDist) <- c(1:nrow(data_tsne))
colnames(tempDist) <- c(1:nrow(data_tsne))
image(tempDist)
plot(tempDist)
# cargo librerias
pacman::p_load(tidyverse, Rtsne, mclust, e1071, cluster)
set.seed(42)
# cargo la data y aplico los mismos tratamientos que en el caso de DBScan
data_cruda  <- read.csv("data/video_games_sales.csv") %>%
mutate(User_Score = as.numeric(User_Score)) %>%
filter(!(is.na(Critic_Score) | is.na(User_Score)))
duplicados <- data_cruda %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
duplicated()
data_sin_dupes  <- data_cruda %>%
filter(!duplicados)
data_tsne <- data_sin_dupes %>%
select(Critic_Score, User_Score, User_Count, Global_Sales) %>%
Rtsne() %>%
.$Y %>%
as.data.frame()
# exploramos graficamente la data
ggplot(data_tsne) +
geom_point(aes(V1,V2))
modelo_kmeans <- kmeans(data_tsne, 13)
## Evaluacion
# inspeccion matriz
#Existen diversos metodos de evaluacion de calidad de los clusters resultantes.
#El primero que revisaremos es la inspeccion visual
# uso distancia euclidiana
tempDist <- dist(data_tsne) %>% as.matrix()
#reordeno filas y columnas en base al cluster obtenido
index <- sort(modelo_kmeans$cluster, index.return=TRUE)
tempDist <- tempDist[index$ix,index$ix]
rownames(tempDist) <- c(1:nrow(data_tsne))
colnames(tempDist) <- c(1:nrow(data_tsne))
? resize
? even
nrow(tempDist)
n <- 2
ids <- nrow(tempDist)/n
ids <- floor(nrow(tempDist)/n)
ids <- 1:floor(nrow(tempDist)/n)
ids <- (1:floor(nrow(tempDist)/n))*n
small_temp <- tempDist[ids,ids]
image(small_temp)
n <- 4
ids <- (1:floor(nrow(tempDist)/n))*n
small_temp <- tempDist[ids,ids]
image(small_temp)
ids <- (1:floor(nrow(tempDist)/n))*n
small_temp <- tempDist[ids,ids]
image(small_temp)
n <- 10
ids <- (1:floor(nrow(tempDist)/n))*n
small_temp <- tempDist[ids,ids]
image(small_temp)
#Correlation
#construyo matriz de correlacion ideal (cada entidad correlaciona 1 con su cluster)
tempMatrix <- matrix(0, nrow = nrow(data_tsne), ncol = nrow(data_tsne))
tempMatrix
View(tempMatrix)
tempMatrix[which(index$x==1), which(index$x==1)]  <- 1
tempMatrix[which(index$x==2), which(index$x==2)]  <- 1
tempMatrix[which(index$x==3), which(index$x==3)]  <- 1
tempMatrix[which(index$x==4), which(index$x==4)]  <- 1
tempMatrix[which(index$x==5), which(index$x==5)]  <- 1
tempMatrix[which(index$x==6), which(index$x==6)]  <- 1
tempMatrix[which(index$x==7), which(index$x==7)]  <- 1
tempMatrix[which(index$x==8), which(index$x==8)]  <- 1
tempMatrix[which(index$x==9), which(index$x==9)]  <- 1
tempMatrix[which(index$x==10), which(index$x==10)] <- 1
View(tempMatrix)
index
unique(index$x)
#Correlation
#construyo matriz de correlacion ideal (cada entidad correlaciona 1 con su cluster)
tempMatrix <- matrix(0, nrow = nrow(data_tsne), ncol = nrow(data_tsne))
for(k in unique(index$x)){
tempMatrix[which(index$x==k), which(index$x==k)]  <- 1
}
#construyo matriz de disimilitud
tempDist2 <- 1/(1+tempDist)
#Calcula correlacion
cor <- cor(tempMatrix[upper.tri(tempMatrix)],tempDist2[upper.tri(tempDist2)])
print(cor)
#construyo matriz de disimilitud
tempDist2 <- 1/(1+tempDist)
#Calcula correlacion
cor <- cor(tempMatrix[upper.tri(tempMatrix)],tempDist2[upper.tri(tempDist2)])
print(cor)
library(factoextra)
#Calcula el hopkins statistic
res <- get_clust_tendency(data_tsne, n = 30, graph = FALSE)
? get_clust_tendency
print(res)
#Cohesion
withinCluster <- numeric(13)
for (i in 1:13){
tempData <- data_tsne[which(modelo_kmeans$cluster == i),]
withinCluster[i] <- sum(dist2(tempData,colMeans(tempData))^2)
}
# cargo librerias
pacman::p_load(tidyverse, Rtsne, mclust, e1071, cluster, flexclust,
factoextra)
#Cohesion
withinCluster <- numeric(13)
for (i in 1:13){
tempData <- data_tsne[which(modelo_kmeans$cluster == i),]
withinCluster[i] <- sum(dist2(tempData,colMeans(tempData))^2)
}
cohesion = sum(withinCluster)
#es equivalente a model$tot.withinss en k-means
print(c(cohesion, modelo_kmeans$tot.withinss))
#Separation
meanData <- colMeans(data_tsne)
SSB <- numeric(13)
#Separation
meanData <- colMeans(data_tsne)
SSB <- numeric(13)
for (i in 1:13){
tempData <- data_tsne[which(modelo_kmeans$cluster==i),]
SSB[i] <- nrow(tempData)*sum((meanData-colMeans(tempData))^2)
}
separation = sum(SSB)
print(separation)
coefSil <- silhouette(modelo_kmeans$cluster,dist(data_tsne))
summary(coefSil)
#visualizamos el codigo de silueta de cada cluster
fviz_silhouette(coefSil) + coord_flip()
coefSil=numeric(30)
for (k in 2:30){
modelo <- kmeans(data_tsne, centers = k)
temp <- silhouette(modelo$cluster,dist(data_escala))
coefSil[k] <- mean(temp[,3])
}
coefSil=numeric(30)
for (k in 2:30){
modelo <- kmeans(data_tsne, centers = k)
temp <- silhouette(modelo$cluster,dist(data_tsne))
coefSil[k] <- mean(temp[,3])
}
data_sin_dupes$Rating
data_sin_dupes$Rating %>% table
coefSil
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:30))
tempDF=data.frame(CS=coefSil,K=c(1:30))
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:30))
for (k in 1:30){
modelo <- kmeans(data_tsne, centers = k)
temp <- silhouette(modelo$cluster,dist(data_tsne))
coefSil[k] <- mean(temp[,3])
}
coefSil=numeric(30)
for (k in 1:30){
modelo <- kmeans(data_tsne, centers = k)
temp <- silhouette(modelo$cluster,dist(data_tsne))
coefSil[k] <- mean(temp[,3])
}
temp
coefSil=numeric(30)
for (k in 2:30){
modelo <- kmeans(data_tsne, centers = k)
temp <- silhouette(modelo$cluster,dist(data_tsne))
coefSil[k] <- mean(temp[,3])
}
tempDF=data.frame(CS=coefSil,K=c(1:30))
# ejecutamos un modelo para probar su clusterizacion
modelo_kmeans <- kmeans(data_tsne, 13)
? get_clust_tendency
#Calcula el hopkins statistic
(res <- get_clust_tendency(data_tsne, n = 30, graph = FALSE))
#Calcula el hopkins statistic
(res <- get_clust_tendency(data_tsne, n = 5, graph = FALSE))
#Calcula el hopkins statistic
(res <- get_clust_tendency(data_tsne, n = 2, graph = FALSE))
#Calcula el hopkins statistic
(res <- get_clust_tendency(data_tsne, n = 100, graph = FALSE))
# almacenamos los clusters en un vector que contiene el numero de cada cluster y el indice
clusters <- sort(modelo_kmeans$cluster, index.return=TRUE)
clusters$ix
? sort
clusters$x
View(clusters)
modelo_kmeans$cluster
# almacenamos los clusters en un vector que contiene el numero de cada cluster y el indice
clusters <- modelo_kmeans$cluster
clusters$index <-  sort(clusters, index.return=TRUE)
# almacenamos los clusters en un vector que contiene el numero de cada cluster y el indice
clusters <- modelo_kmeans$cluster
clusters_i <-  sort(clusters, index.return=TRUE)
# calculamos las distancias de los datos
distancias <- dist(data_tsne) %>% as.matrix()
# generamos indices de los clusters
clusters_i <-  sort(clusters, index.return=TRUE)
clusters_i
clusters
clusters_i
# generamos indices con la ubicacion de los clusters en la base
clusters_i <-  sort(clusters, index.return=TRUE)
#reordeno filas y columnas en base al cluster obtenido
distancias <- distancias[clusters_i$ix, clusters_i$ix]
rownames(distancias) <- c(1:nrow(data_tsne))
colnames(distancias) <- c(1:nrow(data_tsne))
? size
? object.size
object.size(distancias)
object.size(distancias, units = "b")
print(object.size(distancias), units = "b")
print(object.size(distancias), units = "Mb")
# la extraemos 1 de cada 10 filas y columnas
n <- 10
ids <- (1:floor(nrow(distancias)/n))*n
dist_reducida <- distancias[ids,ids]
image(dist_reducida)
print(object.size(dist_reducida), units = "Mb")
# creo matriz llega de ceros
matriz_ideal <- matrix(0, nrow = nrow(data_tsne), ncol = nrow(data_tsne))
# para cada cluster reemplazo con 1 en aquellas entidades que pertenezcan a el
for(k in unique(clusters_i$x)){
matriz_ideal[which(clusters_i$x==k), which(clusters_i$x==k)]  <- 1
}
#construyo matriz de disimilitud en base a la distancia ordenada
tempDist2 <- 1/(1+distancias)
#Calcula correlacion entre matriz de disimilitud y matriz optima
cor <- cor(matriz_ideal[upper.tri(matriz_ideal)],tempDist2[upper.tri(tempDist2)])
print(cor)
# creamos vector vacio para medir la cohesion en cada cluster
withinCluster <- numeric(length(unique(clusters)))
# creamos vector vacio para medir la cohesion en cada cluster
withinCluster <- numeric(length(unique(clusters)))
# para cada cluster
for (i in 1:length(withinCluster)){
tempData <- data_tsne[which(clusters == i),]
withinCluster[i] <- sum(dist2(tempData,colMeans(tempData))^2)
}
# para cada cluster
for (i in 1:length(withinCluster)){
# extraigo los puntos pertenecientes al cluster i
tempData <- data_tsne[which(clusters == i),]
# calculo la suma de distancias al cuadrado entre cada punto y el centroide
withinCluster[i] <- sum(dist2(tempData,colMeans(tempData))^2)
}
# calculo la suma total de cohesion para todos los clusters
cohesion <- sum(withinCluster)
#es equivalente a model$tot.withinss en k-means
print(c(cohesion, modelo_kmeans$tot.withinss))
centroide_total <- colMeans(data_tsne)
centroide_total
# creamos vector vacio para almacenar separacion en cada cluster
separation <-  numeric(length(unique(clusters)))
# creamos vector vacio para almacenar separacion en cada cluster
separation <-  numeric(length(unique(clusters)))
# para cada cluster
for (i in 1:length(separation)){
# extraigo los puntos pertenecientes al cluster i
tempData <- data_tsne[which(clusters==i),]
# calculo la separacion como la distancia promedio entre cada punto de un cluster y el resto
separation[i] <- nrow(tempData)*sum((meanData-colMeans(tempData))^2)
}
(sum(separation))
# el coeficiente recibe las etiquetas de cluster y la matriz de distancias de la data original
coefSil <- silhouette(clusters,dist(data_tsne))
summary(coefSil)
#visualizamos el codigo de silueta de cada cluster
fviz_silhouette(coefSil) + coord_flip()
# Utilizamos el coeficiente de silueta para encontrar el mejor valor de K
# creamos vector vacio para almacenar siluetas
coefSil <- numeric(30)
coefSil[,3]
coefSil
# el coeficiente recibe las etiquetas de cluster y la matriz de distancias de la data original
coefSil <- silhouette(clusters,dist(data_tsne))
summary(coefSil)
# Utilizamos el coeficiente de silueta para encontrar el mejor valor de K
# creamos vector vacio para almacenar siluetas
siluetas <- numeric(30)
for (k in 2:30){
# ejecutamos kmedias con k centroides
modelo <- kmeans(data_tsne, centers = k)
# cramos objeto con la silueta
temp <- silhouette(modelo$cluster, dist(data_tsne))
# almacenamos la silueta promedio del modelo
siluetas[k] <- mean(temp[,3])
}
tempDF=data.frame(CS=siluetas,K=c(1:30))
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:30))
# Utilizamos el coeficiente de silueta para encontrar el mejor valor de K
# creamos vector vacio para almacenar siluetas
siluetas <- numeric(100)
for (k in 2:100){
# ejecutamos kmedias con k centroides
modelo <- kmeans(data_tsne, centers = k)
# cramos objeto con la silueta
temp <- silhouette(modelo$cluster, dist(data_tsne))
# almacenamos la silueta promedio del modelo
siluetas[k] <- mean(temp[,3])
}
tempDF <- data.frame(CS=siluetas, K=c(1:30))
# visualizamos
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:30))
tempDF <- data.frame(CS=siluetas, K=c(1:100))
# visualizamos
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:30))
# visualizamos
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:100))
# Utilizamos el coeficiente de silueta para encontrar el mejor valor de K
# creamos vector vacio para almacenar siluetas
siluetas <- numeric(20)
for (k in 2:20){
# ejecutamos kmedias con k centroides
modelo <- kmeans(data_tsne, centers = k)
# cramos objeto con la silueta
temp <- silhouette(modelo$cluster, dist(data_tsne))
# almacenamos la silueta promedio del modelo
siluetas[k] <- mean(temp[,3])
}
tempDF <- data.frame(CS=siluetas, K=c(1:20))
# visualizamos
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:20))
# visualizamos
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:20)) +
geom_vline(xintercept = max(tempDF$CS))
max(tempDF$CS)
which(tempDF$CS == max(tempDF$CS))
# visualizamos
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:20)) +
geom_vline(xintercept = which(tempDF$CS == max(tempDF$CS)))
source("C:/Users/raimundo.sanchez/Dropbox/01.Docencia/mineria_de_datos/Mineria-de-datos/R/06.evaluacion_clusters.R", encoding = 'UTF-8', echo=TRUE)
# visualizamos
ggplot(tempDF, aes(x=K, y=CS)) +
geom_line() +
scale_x_continuous(breaks=c(1:20)) +
geom_vline(xintercept = which(tempDF$CS == max(tempDF$CS)), col = "red")
